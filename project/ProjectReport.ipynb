{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50c7d78-e6b1-41e9-8c68-182cc6104fc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4336b-1832-4f7c-ae7d-9308573f394e",
   "metadata": {},
   "source": [
    "## Project Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cedad-b79c-47f0-9a22-3556484cb8a9",
   "metadata": {},
   "source": [
    "The output of this project (Data-Preprocessing Module) contains following files:  \n",
    "   - **Data-Preprocessing Module.ipynb**  \n",
    "Documentation and main workflow code of this module, from data import, data processing to data augmentation.  \n",
    "   - **functions.py**  \n",
    "4 newly written functions for building the functionality of this module.\n",
    "   - **dependencies.py**  \n",
    "Import dependencies for the module.\n",
    "   - **test_functions.py**  \n",
    "Test the functions written in *functions.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f0d87-3bee-4c1c-b1cb-a59feb27cdec",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "To achieve the preprocessing task for deep-learning model, this project completes three main functions, more details are shown in the documentation in `Data-Preprocessing Module.ipynb`:\n",
    "\n",
    "### Data import\n",
    "\n",
    "In this section, import images are resized and cropped to keep the most important part of images. However, we can't determine a uniform method for all datasets and models. The users might need to change the size range and cropping range before using it.\n",
    "\n",
    "### Data processing\n",
    "\n",
    "This section uses a lot of exising functions like **to_categorical()** and **train_test_split()** to achieve various processing methods. The functionality of subtracting mean RGB values is built by meself since the methods provided in existing packages are quite different and it's hard to determine which one is consistent with my aim.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "This section calls a very useful function **ImageDataGenerator()** from `tensorflow.keras` to build my own fucntion. After test, only a part of its arguments are suitable to cloud image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bc689-cf81-4d1b-9cae-2f67f3145fb8",
   "metadata": {},
   "source": [
    "## Instructions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63cb65-df7e-43c5-b451-fc58abca5640",
   "metadata": {},
   "source": [
    "This project aims to build a module that achieves the functionality of **cloud image** data import, processing and augmentation for the training and testing of **deep-learning models**.  \n",
    "\n",
    "The dara-preprocessing module for cloud images consists of three parts, which are data import, data processing and data augmentation built on some exising and new functions.  \n",
    "\n",
    " - For data import, the directory of cloud data should be provided. We can also set an index for random shuffling. This part should be slightly modified in advance if a new folder has different structure from the default.  \n",
    " \n",
    " - For data processing, two values between 0 and 1 should be provided, which are used to split training, test and validation sets.  \n",
    " \n",
    " - For data augmentation, what augmentation methods for cloud images are applied should be assigned, which includes ZCA whitening, rotation, width/height shift and horizontal/vertical flip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8171c-72aa-4e6d-96b5-e7785e9bde28",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2289c60-5c9b-43c9-9081-2e107a707ed3",
   "metadata": {},
   "source": [
    "This module uses functions from packages `sklearn` and `tensorflow` to build the functionality of data processing and augmentation, respectively. Package `numpy` is imported to do array calculation and `matplotlib` is imported to plot some image examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055b4185-8d7d-4d66-aa50-65eac7c7ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Import dependencies for the notebook\"\"\"\n",
      "\n",
      "import os\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "import cv2\n",
      "\n",
      "from tqdm import tqdm\n",
      "\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open(\"dependencies.py\")\n",
    "lines = f.read()\n",
    "print(lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64e538-5035-4764-a112-76831d7af98a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc545e48-ad5b-4c9b-ad20-278186ea537c",
   "metadata": {},
   "source": [
    "4 test functions in *test_functions.py* are defined to test each written function in `functions.py` respectively.  \n",
    "\n",
    "- **get_images** is tested with the type of returned cloud class array and if the number of imported labels and images is equal.  \n",
    "- **display_random_image** is tested with the type of returned figure variable.  \n",
    "\n",
    "- **subtract_meanRGB** is tested with the mean R value of the returned array, which should be 0.  \n",
    "\n",
    "- **Image_Generator** is tested with the type of returned data generator variable.\n",
    "\n",

    "We can see test results in the final section of [`Data-Preprocessing Module.ipynb`](https://github.com/JiarunZhou/EMSC-4033-2022_Project_Jiarun/blob/f72bfc51e4ae72507af11fa183835b6e69a53bce/Data-Preprocessing%20Module.ipynb)."

   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905010d-18f6-4233-9e4a-bd45198c9391",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9d855-b3fe-426d-b94c-300805f4e3ee",
   "metadata": {},
   "source": [
    "This project focuses on the preprocessing of cloud images for deep learning models. The output from this project including data-preprocessing module and fucntions can be applied to the task for other objects easily in the future, since in most cases similar procedures of import, processing and augmentation should be went through before inputing the data into deep learning models. To complete this work, we need to determine the best cropping range and image shape, and add new data augmentation methods. However, these changes are not easy to determine since datasets and models can be very different actually. This problem limits the applied range of this module.\n",
    "\n",
    "In this project, some test_functions are defined to test the returns of 4 new written functions. They have determined the functions run without mistakes and return expected variables. However, the finally returned data generators should be applied into a true training task of deep learning models in order to examine the actual performance of this data-preprocessing module, which is impossible to test by test_fucntions. This is an important future work for improve the module and find better data-processing methods. For example:\n",
    "\n",
    "```python\n",
    "#Fit\n",
    "model_1 = load_model(\"model_CloudNet_KAxis\")\n",
    "hist_1 = model_1.fit(TrainGen,\n",
    "                        steps_per_epoch = x_train.shape[0] // 8,\n",
    "                        validation_data = ValGen,\n",
    "                        validation_steps = x_val.shape[0]// 8,\n",
    "                        epochs = 200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e779d-a8ab-4814-8f6e-a64f767d0a4f",
   "metadata": {},
   "source": [
    "## Records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b836620-b19b-4201-8771-72c808edbee0",
   "metadata": {},
   "source": [
    "### Issues\n",
    "\n",
    " - When I was writing the background part in my project planner, I attempted to citing literatures from BibTex file into the markdown file. However, I didn't figure out how to do this operation finally. Therefore, I had to input the reference information manually.   \n",
    "   \n",
    " - In the function for data import, I wanted the fucntion to read the folder name as cloud classes rather than rely on manual input. This requires more loops in imprting process and caused some issues at the beginning. I ssolved this problem using three loop layers finally.  \n",
    " - The input images should be resized to a defined resoulution. If I only resize the images, needless black borders exist; If I only crop them, the information in obtianed images are too limited. Therefore, I chose to first resize the raw images to an appropriate resoulution and then do cropping operation to get required size.  \n",
    " - I originally wanted to plot 25 image samples. However, I noticed there might be less than 25 images in a dataset, thus I added some exception handling to solve this issue.  \n",
    " - There are a lot of data augmentation methods can be done. This is an issue to determine which operations are suitable to cloud images. Finally, I chose 4 operations here: ZCA whitening, rotation, flip and shift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac709e2-976e-40b5-9753-3ae1c0acfd41",
   "metadata": {},
   "source": [
    "### Progress record\n",
    "\n",
    "09/05\n",
    " - Completed `ProjectPlanner.md`.\n",
    " \n",
    "11/05\n",
    " - Completed `Data-Preprocessing Module.ipynb` and determined the functions that should be written by myself.\n",
    " \n",
    "13/05\n",
    " - Completed `functions.py` and `test_functions.py`.\n",
    " \n",
    "18/05\n",
    " - Added examples into docstrings in `functions.py` and restructured the repo.\n",
    " \n",
    "19/05\n",
    " - Added exception handling to `functions.py`.\n",
    " \n",
    "20/05\n",
    " - Completed `README.md` and modified `ProjectReport.ipynb`.\n",
    " \n",
    "22/05\n",
    " - Deleted some cache files on GitHub and released the version 1.0.\n",
    " \n",
    "25/05\n",
    " - Modified `ProjectReport.ipynb` and released the version 1.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
